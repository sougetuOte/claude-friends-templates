#!/usr/bin/env python3
"""
Refactoring Analyzer
æŠ€è¡“è² å‚µã‚’æ¤œå‡ºã—ã€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã®å„ªå…ˆåº¦ã‚’ç®—å‡ºã™ã‚‹
"""

import os
import json
import re
import fnmatch
from datetime import datetime


class RefactoringAnalyzer:
    def __init__(self, config_path=".claude/refactoring-config.json"):
        self.config = self.load_config(config_path)
        self.results = {
            "summary": {},
            "critical": [],
            "high": [],
            "medium": [],
            "low": [],
            "metrics": {},
        }

    def load_config(self, config_path):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰"""
        default_config = {
            "thresholds": {
                "complexity": {
                    "cyclomatic": 10,
                    "cognitive": 15,
                    "maxLines": 100,
                    "maxParams": 5,
                },
                "duplication": {"minLines": 20, "threshold": 0.8},
                "coverage": 80,
            },
            "exclude": [
                "vendor/",
                "node_modules/",
                "build/",
                "dist/",
                "*.min.js",
                "*.generated.*",
                "__pycache__",
                ".git",
            ],
        }

        if os.path.exists(config_path):
            with open(config_path, "r") as f:
                user_config = json.load(f)
                # ãƒãƒ¼ã‚¸
                default_config.update(user_config)

        return default_config

    def should_exclude(self, file_path):
        """é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        for pattern in self.config["exclude"]:
            if pattern.endswith("/"):
                if pattern in file_path:
                    return True
            elif "*" in pattern:
                if fnmatch.fnmatch(file_path, pattern):
                    return True
            elif pattern in file_path:
                return True
        return False

    def calculate_cyclomatic_complexity(self, code):
        """å¾ªç’°çš„è¤‡é›‘åº¦ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        complexity = 1
        # åˆ¶å¾¡æ§‹é€ ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        control_keywords = [
            r"\bif\b",
            r"\belif\b",
            r"\belse\b",
            r"\bfor\b",
            r"\bwhile\b",
            r"\btry\b",
            r"\bcatch\b",
            r"\bcase\b",
            r"\b\?\s*:",
            r"\&\&",
            r"\|\|",
        ]

        for keyword in control_keywords:
            complexity += len(re.findall(keyword, code))

        return complexity

    def count_parameters(self, code):
        """é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        # Pythoné–¢æ•°
        py_func_pattern = r"def\s+\w+\s*\(([^)]*)\)"
        # JavaScripté–¢æ•°
        js_func_pattern = r"function\s+\w+\s*\(([^)]*)\)"
        js_arrow_pattern = r"(?:const|let|var)\s+\w+\s*=\s*\(([^)]*)\)\s*=>"

        max_params = 0
        for pattern in [py_func_pattern, js_func_pattern, js_arrow_pattern]:
            matches = re.findall(pattern, code)
            for match in matches:
                if match.strip():
                    params = len([p.strip() for p in match.split(",") if p.strip()])
                    max_params = max(max_params, params)

        return max_params

    def find_duplicates(self, files_content):
        """ã‚³ãƒ¼ãƒ‰é‡è¤‡ã‚’æ¤œå‡º"""
        duplicates = []
        # ç°¡æ˜“çš„ãªé‡è¤‡æ¤œå‡ºï¼ˆå®Ÿéš›ã¯ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ï¼‰
        code_blocks = {}

        for file_path, content in files_content.items():
            lines = content.split("\n")
            for i in range(
                len(lines) - self.config["thresholds"]["duplication"]["minLines"]
            ):
                block = "\n".join(
                    lines[i : i + self.config["thresholds"]["duplication"]["minLines"]]
                )
                block_hash = hash(block.strip())

                if block_hash in code_blocks:
                    duplicates.append(
                        {
                            "file1": code_blocks[block_hash]["file"],
                            "line1": code_blocks[block_hash]["line"],
                            "file2": file_path,
                            "line2": i + 1,
                            "lines": self.config["thresholds"]["duplication"][
                                "minLines"
                            ],
                        }
                    )
                else:
                    code_blocks[block_hash] = {"file": file_path, "line": i + 1}

        return duplicates

    def calculate_priority_score(self, complexity, impact, frequency, effort):
        """å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        return (impact * frequency * complexity) / max(effort, 1)

    def analyze_file(self, file_path):
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            lines = content.split("\n")
            line_count = len(lines)

            # è¤‡é›‘åº¦è¨ˆç®—
            complexity = self.calculate_cyclomatic_complexity(content)
            params = self.count_parameters(content)

            issues = []

            # è¤‡é›‘åº¦ãƒã‚§ãƒƒã‚¯
            if complexity > self.config["thresholds"]["complexity"]["cyclomatic"]:
                issues.append(
                    {
                        "type": "complexity",
                        "severity": "high",
                        "message": f"å¾ªç’°çš„è¤‡é›‘åº¦ãŒé«˜ã„: {complexity}",
                        "score": complexity,
                    }
                )

            # è¡Œæ•°ãƒã‚§ãƒƒã‚¯
            if line_count > self.config["thresholds"]["complexity"]["maxLines"]:
                issues.append(
                    {
                        "type": "length",
                        "severity": "medium",
                        "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒé•·ã™ãã‚‹: {line_count}è¡Œ",
                        "score": line_count
                        / self.config["thresholds"]["complexity"]["maxLines"],
                    }
                )

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãƒã‚§ãƒƒã‚¯
            if params > self.config["thresholds"]["complexity"]["maxParams"]:
                issues.append(
                    {
                        "type": "parameters",
                        "severity": "medium",
                        "message": f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹: {params}å€‹",
                        "score": params,
                    }
                )

            return {
                "file": file_path,
                "metrics": {
                    "complexity": complexity,
                    "lines": line_count,
                    "parameters": params,
                },
                "issues": issues,
            }

        except (IOError, UnicodeDecodeError):
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ãªã©ï¼‰
            return None

    def analyze_project(self, root_path="."):
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’åˆ†æ"""
        files_content = {}
        file_issues = []

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†ãƒ»åˆ†æ
        for root, dirs, files in os.walk(root_path):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            dirs[:] = [
                d for d in dirs if not self.should_exclude(os.path.join(root, d))
            ]

            for file in files:
                file_path = os.path.join(root, file)

                # é™¤å¤–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if self.should_exclude(file_path):
                    continue

                # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
                if file.endswith((".py", ".js", ".ts", ".java", ".go", ".rb")):
                    result = self.analyze_file(file_path)
                    if result and result["issues"]:
                        file_issues.append(result)

                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            files_content[file_path] = f.read()
                    except (IOError, UnicodeDecodeError):
                        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
                        pass

        # é‡è¤‡æ¤œå‡º
        duplicates = self.find_duplicates(files_content)

        # çµæœã‚’å„ªå…ˆåº¦åˆ¥ã«åˆ†é¡
        for issue_data in file_issues:
            for issue in issue_data["issues"]:
                # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                score = self.calculate_priority_score(
                    complexity=issue.get("score", 1),
                    impact=3,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    frequency=2,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    effort=1,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                )

                recommendation = {
                    "file": issue_data["file"],
                    "issue": issue["message"],
                    "type": issue["type"],
                    "score": score,
                    "effort": "1æ™‚é–“",  # ç°¡æ˜“è¦‹ç©ã‚‚ã‚Š
                    "recommendation": self.get_recommendation(issue["type"]),
                }

                if score > 15:
                    self.results["critical"].append(recommendation)
                elif score > 10:
                    self.results["high"].append(recommendation)
                elif score > 5:
                    self.results["medium"].append(recommendation)
                else:
                    self.results["low"].append(recommendation)

        # é‡è¤‡ã‚’çµæœã«è¿½åŠ 
        for dup in duplicates:
            self.results["high"].append(
                {
                    "file": dup["file1"],
                    "issue": f"{dup['file2']}ã¨{dup['lines']}è¡Œã®é‡è¤‡",
                    "type": "duplication",
                    "score": 12,
                    "effort": "30åˆ†",
                    "recommendation": "å…±é€šé–¢æ•°ã¨ã—ã¦æŠ½å‡º",
                }
            )

        # ã‚µãƒãƒªãƒ¼ä½œæˆ
        self.results["summary"] = {
            "total_files": len(files_content),
            "critical_count": len(self.results["critical"]),
            "high_count": len(self.results["high"]),
            "medium_count": len(self.results["medium"]),
            "low_count": len(self.results["low"]),
            "timestamp": datetime.now().isoformat(),
        }

        return self.results

    def get_recommendation(self, issue_type):
        """å•é¡Œã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæ¨å¥¨äº‹é …ã‚’è¿”ã™"""
        recommendations = {
            "complexity": "é–¢æ•°ã‚’åˆ†å‰²ã—ã€æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨",
            "length": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åˆ†å‰²",
            "parameters": "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨",
            "duplication": "å…±é€šå‡¦ç†ã‚’é–¢æ•°ã¨ã—ã¦æŠ½å‡º",
        }
        return recommendations.get(issue_type, "ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚’æ¤œè¨")

    def generate_report(self, output_format="markdown"):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if output_format == "markdown":
            return self.generate_markdown_report()
        elif output_format == "json":
            return json.dumps(self.results, indent=2, ensure_ascii=False)
        else:
            return str(self.results)

    def generate_markdown_report(self):
        """Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = f"""# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {self.results['summary']['timestamp']}

## ã‚µãƒãƒªãƒ¼
- åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.results['summary']['total_files']}
- Critical: {self.results['summary']['critical_count']}ä»¶
- High: {self.results['summary']['high_count']}ä»¶
- Medium: {self.results['summary']['medium_count']}ä»¶
- Low: {self.results['summary']['low_count']}ä»¶

"""

        # Critical
        if self.results["critical"]:
            report += "## ğŸ”´ Criticalï¼ˆå³åº§ã«å¯¾å¿œï¼‰\n\n"
            for item in self.results["critical"]:
                report += f"- [ ] `{item['file']}` - {item['issue']}\n"
                report += f"  - æ¨å¥¨: {item['recommendation']}\n"
                report += f"  - è¦‹ç©ã‚‚ã‚Š: {item['effort']}\n\n"

        # High
        if self.results["high"]:
            report += "## ğŸŸ  Highï¼ˆä»Šã‚¹ãƒ—ãƒªãƒ³ãƒˆå†…ï¼‰\n\n"
            for item in self.results["high"][:5]:  # ä¸Šä½5ä»¶ã®ã¿
                report += f"- [ ] `{item['file']}` - {item['issue']}\n"
                report += f"  - æ¨å¥¨: {item['recommendation']}\n"
                report += f"  - è¦‹ç©ã‚‚ã‚Š: {item['effort']}\n\n"

        # Mediumï¼ˆä»¶æ•°ã®ã¿ï¼‰
        if self.results["medium"]:
            report += "\n## ğŸŸ¡ Mediumï¼ˆæ¬¡ã‚¹ãƒ—ãƒªãƒ³ãƒˆæ¤œè¨ï¼‰\n\n"
            report += f"{len(self.results['medium'])}ä»¶ã®æ”¹å–„å€™è£œãŒã‚ã‚Šã¾ã™ã€‚\n\n"

        # Lowï¼ˆä»¶æ•°ã®ã¿ï¼‰
        if self.results["low"]:
            report += "## ğŸŸ¢ Lowï¼ˆæ™‚é–“ãŒã‚ã‚‹ã¨ãï¼‰\n\n"
            report += f"{len(self.results['low'])}ä»¶ã®è»½å¾®ãªæ”¹å–„å€™è£œãŒã‚ã‚Šã¾ã™ã€‚\n"

        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse

    parser = argparse.ArgumentParser(description="ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--path", default=".", help="åˆ†æå¯¾è±¡ã®ãƒ‘ã‚¹")
    parser.add_argument(
        "--format",
        choices=["markdown", "json"],
        default="markdown",
        help="å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ",
    )
    parser.add_argument("--output", help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæŒ‡å®šãªã—ã¯æ¨™æº–å‡ºåŠ›ï¼‰")

    args = parser.parse_args()

    analyzer = RefactoringAnalyzer()
    analyzer.analyze_project(args.path)
    report = analyzer.generate_report(args.format)

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ {args.output} ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
    else:
        print(report)


if __name__ == "__main__":
    main()
